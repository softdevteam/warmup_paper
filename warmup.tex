\documentclass[preprint,numbers,10pt]{sigplanconf}

\usepackage[utf8]{inputenc}
% the following standard packages may be helpful, but are not required
%\usepackage{longtable}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{courier}
\usepackage[scaled]{helvet}
\usepackage{url}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mdwlist} % tighter description environment (starred)
\usepackage{gensymb}

\usepackage{graphicx}
\usepackage{softdev}
\usepackage{amsmath}
\usepackage{mdwlist}
\usepackage{pifont}
\usepackage{xspace}

\newcommand{\kalibera}{Kalibera \& Jones\xspace}
\newcommand{\krun}{Krun\xspace}
\newcommand{\hypone}{H1\xspace}
\newcommand{\hyptwo}{H2\xspace}
\newcommand{\binarytrees}{\emph{binary trees}\xspace}
\newcommand{\richards}{\emph{Richards}\xspace}
\newcommand{\spectralnorm}{\emph{spectralnorm}\xspace}
\newcommand{\nbody}{\emph{n-body}\xspace}
\newcommand{\fasta}{\emph{fasta}\xspace}
\newcommand{\fannkuch}{\emph{fannkuch redux}\xspace}
\newcommand{\bencherthree}{Linux1/i7-4709K\xspace}
\newcommand{\bencherfive}{Linux2/i7-4790\xspace}
\newcommand{\benchersix}{OpenBSD/i7-4790\xspace}
\newcommand{\bencherseven}{ARM\edd{XXX name properly if used}\xspace}

\lstset{
    basicstyle=\tt\scriptsize,
    xleftmargin=2em,
    framexleftmargin=1.5em,
    numberstyle=\scriptsize\tt\color{gray},
    captionpos=b,
    escapeinside={{<!}{!>}},
}

\SDShowCommentTags{default}  %final

\begin{document}

\title{Virtual Machine Warmup Blows Hot and Cold}

\authorinfo{Edd Barrett}{King's College London}{\texttt{http://eddbarrett.co.uk/}}
\authorinfo{Carl Friedrich Bolz}{King's College London}{\texttt{http://cfbolz.de/}}
\authorinfo{Rebecca Killick}{University of Lancaster}{\texttt{http://www.lancs.ac.uk/\~{}killick/}}
\authorinfo{Sarah Mount}{King's College London}{\texttt{http://snim2.org/}}
\authorinfo{Laurence Tratt}{King's College London}{\texttt{http://tratt.net/laurie/}}
%\keywords{warmup, benchmarking, virtual machines, programming languages.}

%\authorrunning{E. Barrett, C. F. Bolz, R. Killick, V. Knight, S. Mount, and L. Tratt}

\maketitle

\begin{abstract}
Virtual Machines (VMs) with Just-In-Time (JIT) compilers are traditionally thought
to execute programs in two phases: first the \emph{warmup} phase determines which
parts of a program would most benefit from dynamic compilation; after
compilation has occurred, the program is said to be at \emph{peak performance}.
When measuring the performance of JIT compiling VMs, data collected
during the warmup phase is generally discarded, placing the focus on peak
performance. In this paper we first run a number of small,
deterministic benchmarks on a variety of well known VMs, before introducing
a rigorous statistical model for determining when warmup has occurred.
In our experiment, \laurie{only XXX\%} of the benchmark/VM pairs conform to
the traditional notion of warmup, and none of the
VMs we tested consistently warms up in the traditional notion.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Many modern languages are implemented as Virtual Machines (VMs) which use a
Just-In-Time (JIT) compiler to translate `hot' parts of a program into efficient
machine code at run-time. Since it takes time to determine which parts of the
program are hot, and then compile them, programs which are JIT compiled are
said to be subject to a \emph{warmup} phase. The traditional view of
JIT compiled VMs is that program execution is slow during the warmup phase, and
fast afterwards, when a steady state of \emph{peak performance} is said to have been reached
(see Figure~\ref{fig:trad} for a simplified view of this).
This traditional view underlies most benchmarking of JIT compiled VMs, which
usually require running benchmarks several times within a single VM process,
discarding any timing data collected before warmup is complete, and
reporting only peak performance figures.

The fundamental aim of this paper is to test the following hypothesis, which captures a constrained
notion of the traditional notion of warmup:
\begin{description}
  \item[\hypone] Small, deterministic programs exhibit traditional warmup behaviour.
\end{description}
In order to test this hypothesis, we present a carefully designed
experiment where a number of simple benchmarks are run on a variety of
VMs for a large number of \emph{in-process iterations} and repeated using fresh
\emph{process executions} (i.e.~each process execution runs multiple in-process
iterations). We also introduce the first automated approach to determining
\emph{when} warmup has completed, based on changepoint analysis.

\begin{figure}[t]
\centering
\includegraphics[width=.475\textwidth]{img/picturebook_warmup}
\caption{The traditional notion of warmup: a program starts slowly executing in
an interpreter; once hot parts of the program are identified, they are
translated by the JIT compiler to machine code; at this point warmup
is said to have completed, and peak performance reached.}
\label{fig:trad}
\end{figure}

While our results show that some benchmarks on some VMs run as per traditional
expectations, many surprising cases exist: some benchmarks slowdown rather than
warmup; some never hit a steady state; some perform very differently over
different process executions; and many exhibit cyclic behaviour. Of the eight
VMs we looked at, none consistently warmed up as suggested by the traditional model.

Our results thus clearly invalidate Hypothesis H1, showing that the traditional
view of warmup is no longer valid (and, perhaps, that it may not have held in
the past). This is of importance to both VM developers and users, since it means
that much VM benchmarking is likely to be partly misleading: indeed, it is
likely to have allowed some ineffective, and perhaps even some deleterious,
optimisations to make their way into production VMs.

\vspace{10pt}

\noindent In order to test Hypothesis H1, we first present a carefully designed
experiment (Section~\ref{sec:methodology}). We then introduce a new
statistical method for automatically determining if/when warmup has occurred,
which allows us to classify benchmarks' warmup style, and present steady-state
in-process iteration times (when possible) based upon the classifications
(Section~\ref{sec:stats}). Since a number of cases fail to warmup in the
traditional fashion, we then run a smaller experiment to understand how JIT
compilation and Garbage Collection (GC) affect warmup
(Section~\ref{sec:deepdive}). As a useful side bonus of our main experiment,
we present data for VM \emph{startup} time: how long it takes until a VM can
start executing any user code (Section~\ref{sec:startup}). Finally, we show that
our statistical method can be applied to well known benchmark suites run in a
conventional manner (Section~\ref{sec:existing}).


\section{Background}
\label{sec:warmup}

Figure~\ref{fig:trad} shows the expected performance profile of a
program subject to the traditional model of warmup.
When a program begins running on a JIT compiled VM, it is typically (slowly)
interpreted; once `hot' (i.e.~frequently executed) loops or methods are
identified, they are dynamically compiled into machine code; and subsequent
executions of those loops or methods use (fast) machine code rather than the
(slow) interpreter. Once machine code generation has completed, the VM is
said to have finished warming up, and the program to be executing
at a steady state of peak performance.\footnote{This traditional notion applies equally to VMs
that perform immediate compilation instead of using an interpreter, and to
those VMs which have more than one layer of JIT compilation (later JIT
compilation is used for `very hot' portions of a program, and tolerates slower
compilation time for better machine code generation).}
While the length of the warmup period
is dependent on the program and JIT compiler, all JIT compiling
VMs assume this performance model holds true~\cite{kalibera13rigorous}.

Benchmarking of JIT compiled VMs typically focusses on peak
performance, partly due to an assumption that
warmup is both fast and inconsequential to users. With that assumption in mind, the
methodologies used are typically straightforward: benchmarks are run for a number
of in-process iterations within a single VM process execution.
The first $n$ in-process iterations are then discarded, on the basis that warmup
will have completed at some point before $n$. It is common for
$n$ to be hard-coded -- in our experience often to 5 -- for all benchmarks.

One of the obvious flaws in this simple methodology is that there is no guarantee
that warmup has completed by in-process iteration $n$. A more sophisticated VM benchmarking methodology
was developed by \kalibera to solve a number of issues when benchmarking JIT
compiling VMs~\cite{kalibera12quantifying,kalibera13rigorous}. The basic idea is
that, for a given VM / benchmark combination, a human must inspect data obtained by
executing a small number of process executions, and determine at which in-process iteration the
benchmark has definitively warmed up. A larger number of VM process executions are then
run, and the previously determined cut-off point applied to each process's
iterations. The \kalibera methodology observes that some benchmarks do not
obviously warm up; and that others follow cyclic patterns post-warmup
(e.g.~in-process iteration $m$ is slow, $m+1$ is fast, for all even values of $m > n$). In
the latter case, the \kalibera methodology requires a consistent in-process iteration in
the cycle be picked for all process executions, and used for statistical analysis.

Although the \kalibera methodology is the most
sophisticated currently available (see its use in
e.g.~\cite{barrett15approaches,grimmer15dynamically}),
there remain cases where it remains hard to produce
satisfying benchmarking statistics. Crucially, the methodology does not
provide a repeatable way of determining when warmup has completed, nor does it
deal satisfactorily with cyclic data. Because of this
``determining when a system has warmed up, or even providing a
rigorous definition of the term, is an open research problem''~\cite{seaton15phd}.


\section{Methodology}
\label{sec:methodology}

To test Hypothesis H1, we designed an experiment which uses a suite of
micro-benchmarks: each is run with 2000 in-process iterations and repeated
using 10 process executions. We have carefully designed our
experiment to be repeatable and to control as many potentially confounding variables as
is practical. In this section we detail: the benchmarks we used and the modifications we
applied; the machines we used for benchmarking; the VMs we benchmarked; and the
\krun system we developed to run benchmarks.


\subsection{The Micro-benchmarks}

The micro-benchmarks we use are as follows: \binarytrees, \spectralnorm, \nbody,
\fasta, and \fannkuch from the Computer Language Benchmarks Game (CLBG)~\cite{clbg}; and
\richards. Readers can be forgiven for initial scepticism about this set of micro-benchmarks.
They are small and widely
used by VM authors as optimisation targets. In general they are more effectively
optimised by VMs than average programs; when used as a proxy for other types
of programs (e.g.~large programs), they tend to overstate the effectiveness of
VM optimisations (see e.g.~\cite{ratanaworabhan09jsmeter}). In our context, this weakness is in fact a strength:
small, deterministic, and widely examined programs are our most
reliable means of testing Hypothesis H1. Put another way, if we were to run arbitrary programs
and find unusual warmup behaviour, a VM author might reasonably counter that
``you have found the one program that exhibits unusual warmup behaviour''.

For each benchmark, we provide C, Java, Javascript, Python, Lua, PHP,
and Ruby versions. Since most of these
benchmarks have multiple implementations in any given language, we picked
the same versions used in~\cite{bolz14impact}, which represented the fastest
performers at the point of that publication. We lightly modified
the benchmarks to integrate with our benchmark runner (see Section~\ref{krun}).
For the avoidance of doubt we did not interfere with any VM's GC (e.g.~we did not
force a collection after each iteration).


\subsubsection{Ensuring Determinism}

User programs that are deliberately non-deterministic programs are unlikely to
warm-up in the traditional fashion.
We therefore wish to guarantee that our benchmarks are,
at the user's level, deterministic, by which we mean that they
take precisely the same path through the Control Flow Graph (CFG)
on all process executions and in-process iterations. Note that non-determinism
beyond that controllable by the user (i.e.~non-determinism in low-level parts of
the VM) is part of what we need to test for Hypothesis H1.

To check whether our benchmarks were deterministic at the user-level, we created
versions with \texttt{print} statements at all possible points of CFG
divergence (e.g.~\texttt{if} statements' true and false branches).
These versions are available in our experimental suite. We first ran the modified
benchmarks with 2 process executions and 20 in-process iterations,
and compared the outputs of the two processes. This was enough to show that the
\fasta benchmark was non-deterministic
in all language variants, due to its random number generator not being reseeded. We
fixed this by moving the random seed initialisation to the start
of the in-process iteration main loop.

In order to understand whether we might be subject to the effects of link
order and other similar effects (see~\cite{mytkowicz09surprising}),
we then used two different machines to compile VMs and ran our modified benchmarks
on them. We then observed occasional non-determinism in Java benchmarks.
This was due to the extra class we had added to each benchmark
to interface between the benchmark runner and the benchmark: on occasions the
main benchmark class was lazily loaded after benchmark timing had started in a
way that we could observe. We
solved this by adding an empty static method to each benchmark, which our
extra classes then call via a static initialiser, guaranteeing that
the main benchmark class is eagerly loaded. Note that Java
benchmarks -- as well as
Java-based systems such as Graal and JRuby/Truffle -- will still be subject to
lazy loading, which is an inherent part of the JVM specification: forcing all
classes to be eagerly loaded is impractical, and is thus part of testing
Hypothesis H1.


\subsection{Measuring Computation and Not File Performance}

By their very nature, micro-benchmarks tend to perform computations which
can be easily optimised away. While this speaks well of
optimising compilers, benchmarks whose computations
are entirely removed are rarely useful~\cite{seaton15phd}. To avoid this,
many benchmarks write intermediate and final results
to \texttt{stdout}, preventing optimisers from removing the dependant code.
However, one can quickly end up in a situation where benchmarks are
unintentionally measuring, in part or whole, the performance of file routines in
libraries and the kernel.

To avoid this possibility, we modified the benchmarks to calculate a checksum
during each in-process iteration.
The checksum is validated at the end of each in-process iteration against an expected
value; if the check fails, the incorrect checksum is written to \texttt{stdout}.
By writing benchmarks in
this style, we make it difficult for optimising compilers to remove the
main bulk of the benchmark. As a useful bonus, we use a single checksum value
for each benchmark irrespective of language, giving some assurance that each language variant is
performing the same work.


\subsection{VMs under investigation}

We ran the benchmarks on the following language implementations: GCC 4.9.3;
Graal \#9405be47 (an alternative JIT compiler for HotSpot); HHVM 3.14.0 (a JIT
compiling VM for PHP); JRuby/Truffle \#170c9ae6; HotSpot 8u72b15 (the most widely used Java
VM); LuaJIT 2.0.4 (a tracing JIT compiling VM for Lua); PyPy 5.3.0 (a
meta-tracing JIT compiling VM for Python 2.7); and V8 5.1.281.65 (a JIT
compiling VM for JavaScript). A repeatable build script downloads, patches,
and builds fixed versions of each VM. All VMs were compiled with GCC/G++ 4.9.3
(and GCC/G++ bootstraps itself, so that the version we use compiled itself)
to remove possible variable performance introduced by different compilers.

On OpenBSD, we skip Graal, HHVM, and JRuby/Truffle, which have not yet been
ported to OpenBSD. We skip \fasta on JRubyTruffle as it crashes;
and we skip \richards on HHVM since it takes as long as every other benchmark
on every other VM put together (about 10 days).


\subsection{Benchmarking Hardware}

With regards to hardware and operating systems, we started with the
following hypothesis:
\begin{description}
  \item[\hyptwo] Moderately different hardware and operating systems have little effect on warmup.
\end{description}
We deliberately use the word `moderately', since significant changes of hardware
(e.g.~x86 vs.~ARM) or operating system (e.g.~Linux vs.~Windows) imply that
significantly different parts of the VMs will be used (and e.g.~different machine
code backends may not all be equally mature).

In order to test Hypothesis H2, we used three benchmarking machines: \emph{\bencherthree}, a quad-core i7-4790K
4GHz, 24GB of RAM, running Debian 8; \emph{\bencherfive}, a quad-core i7-4790
3.6GHz, 32GB of RAM, running Debian 8; and \emph{\benchersix}, with identical
hardware to \bencherfive, but running OpenBSD 5.8. \bencherthree and \bencherfive
have the same OS (with the same updates etc.) but different hardware; \bencherfive
and \benchersix have the same hardware (to the extent that we can determine)
but different operating systems.

We disabled turbo boost and hyper-threading in the BIOS. Turbo boost is a
feature which allows CPUs to temporarily run in an extremely high-performance
mode; when the CPU's safe limits (e.g.~power draw, or temperature) are exceeded,
performance is partly or wholly reduced. For example, we observed on a 4 core
nominally-3.4GHz machine that when 3 or fewer cores were in use turbo boost
raised performance to 3.8GHz, but that using all 4 cores immediately reduced
performance to 3.6GHz. Turbo boost can thus substantially change one's
perception of performance. Hyper-threading gives the illusion that a single
physical core is in fact more than one logical core, inter-leaving the
execution of two or more programs or threads on a single physical core.
Hyper-threading causes programs to interfere
with each others in complex ways, introducing considerable noise.


\subsection{\krun}
\label{krun}

Many confounding variables occur shortly before, and during the running of,
benchmarks. In order to control as many of these as possible, we wrote
\krun, a new benchmark runner. \krun itself is a `supervisor'
which, given a configuration file specifying VMs, benchmarks (etc.) configures
a Linux or OpenBSD system, runs benchmarks, and collects the results. Individual VMs and benchmarks
are then wrapped, or altered, to report data back to \krun in an appropriate format.

In the remainder of this subsection, we describe \krun. Since most of \krun's
controls work identically on Linux and OpenBSD, we start with those controls,
before detailing the differences imposed by the two operating systems. We then
describe how \krun collects data from benchmarks.
Note that, although \krun has various `developer' flags to aid development
and debugging benchmarking suites, we describe only \krun's full `production' mode.


\subsubsection{Platform Independent Controls}

A typical problem with benchmarking is that earlier process executions can
affect later ones (e.g.~a benchmark which forces memory to swap will make
later benchmarks seem to run slower). Therefore, before each process execution
(including before the first), \krun reboots the system, ensuring that the
benchmark runs with the machine in a (largely) known state. After each reboot, \krun is
executed by the system's init subsystem; \krun then pauses for 3
minutes to allow the system to fully initialise; calls \texttt{sync} (to
flush any remaining files to disk) followed by a 30 second wait; before finally running the
next process execution.

When first run, \krun analyses its configuration file and creates a benchmark
running schedule which is written to a file. After each reboot, it reads this
schedule in, runs the next benchmark, and updates the schedule file. The
schedule file is formatted in such a way that it is always a constant size
\laurie{right?}, so that, to the extent that we can control, \krun runs in
constant memory before benchmark running. Note that after a benchmark has run,
\krun can safely perform arbitrary memory allocation; rebooting the machine
wipes the slate clean again for the next benchmark.

Modern systems have various safety limiters built in. For example,
processors have temperature sensors in; if the processor's safe limit
is exceeded, the processor is moved to a lower-power mode. To limit the extent to which this can
affect our benchmarks, we start each process execution with the machine at a similar temperature.
Before its first process execution, \krun waits for 1 minute, before collecting
the values of all available temperature sensors. After each reboot's \texttt{sync}-wait, \krun waits
for the machine to return to these base temperatures ($\pm3\degree$C) before
starting the benchmark. If any sensor fails to return to this temperature range
after 1 hour, \krun aborts the whole experiment.

\krun imposes a
user-configurable heap and stack \texttt{ulimit} on all VM process executions
(in our case, 2GiB heap and a 8MiB stack).\footnote{Note that Linux allows users
to inspect these values, but to allocate memory beyond them.} Benchmarks are run
as the UNIX user `\texttt{krun}', which is fully deleted (including its home
directory) and then created afresh before each process execution.
\laurie{huh? does this mean `Note that the user has no configuration files
included, and thus performs no environment configuration of its own.'} The
performs no environment configuration.

User-configurable commands can be run before and after benchmark execution. In
our experiment, we switch off as many Unix daemons as possible (e.g.~smtpd,
crond) to lessen the effects of context switching to other processes. We also
turn off network interfaces entirely, to prevent outside sources causing unbounded
(potentially performance interfering) interrupts to be sent to the processor and kernel.

In order to identify problems with the machine itself, \krun monitors the
system's \texttt{dmesg} buffer for unexpected entries (known `safe' entries
are ignored), informing the user if any arise. We implemented this after
noticing that one machine initially ear-marked for benchmarking occasionally
overheated, with the only clue to this being a line in \texttt{dmesg}.
We did not use this machine for our final benchmarking.

\laurie{need to think a bit about this} \edd{citation} suggests that the size of the environment can affect
performance, so \krun also logs the environment under which each process
execution is invoked. We check the environments are the same size for all 10
process executions of each benchmark and VM pairing.


\subsubsection{Linux-specific Controls}

On Linux, \krun controls several additional factors.

\krun sets the CPU frequency to the highest non-over-clocked value possible.
The user must first disable Intel P-state support in
the kernel by passing the kernel argument \texttt{intel\_pstate=disable} \laurie{why? i think it's just because the driver isn't very good?}.
\krun verifies P-states are disabled and uses \texttt{cpufreq-set} to set
the CPU governor to \texttt{performance} mode. Note that even with these
options set, we cannot fully guarantee that the CPU does as requested
(see Section~\ref{sec:threats} for details).

\krun checks that it is running on a `full tickless' kernel, which aims to reduce
jitter in time-sensitive workloads~\cite{tickless}. The default
Linux kernel interrupts each active logical CPU\footnote{Note that each core of
each individual processor chip counts as a logical CPU.} 250 times (`ticks') a second to
decide whether to perform a context switch. We used a kernel with the
\texttt{CONFIG\_NO\_HZ\_FULL\_ALL} compile-time option set, which puts
all CPUs except one (the boot CPU) into adaptive-tick mode.
CPUs in adaptive-tick mode are only interrupted by the kernel if more than
one runnable process is scheduled.
When we compared a subset of benchmarks on a tickless vs.~a standard
kernel, we noticed a small reduction in jitter, although not enough for us to
conclusively say that the tickless kernel was the cause. However,
since, at worst, it appears to have no negative effects, we ran our experiments
using the tickless kernel.

Linux's \texttt{perf} system dynamically profiles system performance by
repeatedly sampling hardware counters. We became aware of \texttt{perf} when
\krun's \texttt{dmesg} checks notified us that the kernel had decreased the
sample-rate as it determined that it was sampling too often. Since \texttt{perf}
can interrupt benchmarking, its existence is undesirable, particularly since its
effects can vary over time. Although \texttt{perf} cannot be disabled entirely,
\krun sets the sample-rate to the smallest possible value of 1 sample per
second.

Finally, \krun disables Address Space Layout Randomisation (ASLR). While this is
a sensible security precaution for everyday use, it introduces obvious
non-determinism between process executions.\footnote{The Stabilizer
system~\cite{curtsinger13stabilizer} is an intriguing approach for obtaining reliable
statistics in the face of features such as ASLR. Unfortunately we were not able
to build it on a modern Linux system.}


\subsubsection{OpenBSD-specific Controls}

Relative to Linux, OpenBSD exposes many fewer knobs to users. Nevertheless,
there are two OpenBSD specific features in \krun.
First, \krun sets CPU performance to maximum by invoking \texttt{apm -H} prior
to running benchmarks. This is equivalent to setting Linux's CPU governor to
\texttt{performance} mode, but note that OpenBSD offers no means of influencing
Intel P-states.
Second, \krun minimises the non-determinism in OpenBSD's malloc implementation,
for example not requiring \texttt{realloc} to always reallocate memory to
an entirely new location. The \texttt{malloc.conf} flags we use are \texttt{sfghjpru}.


\subsubsection{The Iterations Runners}

Since we run benchmarks in several different languages, timing data needs
to be reported back to the main \krun process. Thus for each language, we created an
\emph{in-process iterations runner}. When \krun wants to run a benchmark, it opens a pipe to the
appropriate in-process iterations runner, passing it a benchmark name, and the
desired number of in-process iterations. For each in-process iteration we
measure the wall-clock time taken (on Linux and OpenBSD), core
cycle count deltas and APERF/MPERF ratios (on Linux only).

We use a monotonic wall-clock timer with sub-millisecond accuracy
(\texttt{CLOCK\_MONOTONIC\_RAW} on Linux, and \texttt{CLOCK\_MONOTONIC} on
OpenBSD). Although wall-clock time is the only measure which really matters to
users, it doesn't provide insight into multi-threaded computations. By recording
core cycle counts using the \texttt{CPU\_CLK\_UNHALTED.CORE} counter, we can see
what work each core is actually doing. The APERF counter increments at a fixed
frequency for each instruction executed; the MPERF counter increments at a rate
proportional to the processor's current frequency. With an APERF / MPERF ratio of
precisely 1, the processor is running at full speed; below 1 it is being
throttled; and above 1, turbo boost is being used. The APERF/MPERF ratio
therefore acts as a safety check that our wall-clock times are meaningful. On
Linux, we read these two counters via a special file node, which is relatively
slow; for very short time periods \laurie{need a figure here. is it something
like 0.0001 or smaller?}, the error in reading the two counters can become
noticeable. This is compounded by the fact that when inactive, cores do not
update the counters. Fortunately, all our benchmarks run long enough for this
error to be immaterial on active cores.

\laurie{edd, you used IA32\_MPERF as a name. why IA32? are there other
aperf/mperf counterss on the processors we're using?}

Collection of the above metrics is implemented in a small C library
\texttt{libkruntime.so} which each iterations runner calls using -- if possible
-- a foreign function interface. VMs which do not have the ability to call out
to C code were instead patched to expose our measurement functions at the
language-level. Since some of the measurements are 64-bit unsigned integers,
which some of the languages under investigation do not support. In these cases,
the measurements are converted to double-precision floats, which are checked
for loss of precision. Note that collection of the cycle counts and the
performance
ratios is only possible on Linux at this time.

A deliberate design goal of the in-process iterations runners is to minimise
timing noise and distortion from measurements. We do not make any system calls
other than those required to take measurements. System calls require the CPU to
switch from user mode to kernel mode, before executing code in the kernel; on
Linux, calling functions such as \texttt{write} can evict as much as two thirds
of an x86's L1 cache~\cite{soares10flexsc}, and can increase the chances of a
context switch (which is significantly slower than a mode switch). We avoid
in-benchmark I/O and memory allocation by storing measurements in statically
bounded memory buffers. Only once the final in-process iteration has finished
are the measurements emitted to \texttt{stdout}. \krun then accepts the
measurements in JSON format over a pipe and stores them to disk.
Figure~\ref{lst:pyiter} shows
an elided version of the Python in-process iterations runner.
Listing~\ref{lst:krun-measure} shows the underlying implementation of
\texttt{krun\_measure}.

\begin{lstlisting}[label=lst:pyiter, caption={An elided version of the Python
in-process iterations runner (with core cycles etc. removed).}]
wallclock_times = [0] * iters
for i in xrange(iters):
    krun_measure(0)   # Start timed section
    bench_func(param) # Call the benchmark
    krun_measure(1)   # End timed section
    wallclock_times[i] = \
        krun_get_wallclock(1) - krun_get_wallclock(0)
js = { "wallclock_times": wallclock_times }
sys.stdout.write("%s\n" % json.dumps(js))
\end{lstlisting}

\begin{lstlisting}[label=lst:krun-measure, xleftmargin=0cm,
        caption={%
\texttt{krun\_measure}: Measurements can't be taken atomically, so later
measurements include the time taken to read earlier measurements. Since
wall-clock time is the most important measure, it is innermost; since
the APERF / MPERF ratio is a sanity check, it is outermost. Note that
the APERF / MPERF ratios must be read in the same order both before
and after a benchmark.}]
void krun_measure(int mdata_idx) {
  struct krun_data *data = &(krun_mdata[mdata_idx]);
  if (mdata_idx == 0) { // start benchmark readings
    for (int core = 0; core < num_cores; core++) {
      data->aperf[core] = read_aperf(core);
      data->mperf[core] = read_mperf(core);
      data->core_cycles[core] = read_core_cycles(core);
    }
    data->wallclock = krun_clock_gettime_monotonic();
  } else {              // end benchmark readings
    data->wallclock = krun_clock_gettime_monotonic();
    for (int core = 0; core < num_cores; core++) {
      data->core_cycles[core] = read_core_cycles(core);
      data->aperf[core] = read_aperf(core);
      data->mperf[core] = read_mperf(core);
    }
  }
}
\end{lstlisting}


\section{Automatically Determining when Warmup has Completed}
\label{sec:stats}


\section{Results}
\label{sec:Results}

%           execs    iters
% BSD         350  600,000
% Linux       460  920,000
%
% This excludes the empty benchmark

In total, our experiment runs  1270 unique process executions, giving a total
of 2,440,000 in-process iteration readings.
In this section, we analyse this data.

A full set raw data and plots can be downloaded from:
\edd{XXX bump version when new data is up}
\vspace{-.5em}
\begin{center}
{\small%
\url{https://archive.org/download/softdev_warmup_experiment_artefacts/v0.1/}
}
\end{center}

\subsection{Results at a Glance}

To begin our analysis, for each
process execution we generate a series of plots sharing a common $x$
axis: the sequential in-process execution number. The $y$ axis for a
\textbf{run-sequence plot} is the wall-clock time delta (in seconds). The $y$
axis for a \textbf{core-cycle plot} is the cycle count delta for an individual
CPU core. The $y$ axis for a \textbf{frequency ratio plot} plot is the
\texttt{IA32\_APERF}/\texttt{IA32\_AMERF} delta ratio for an individual CPU
core. The latter two plots are per-core, so for these we plot one for each CPU
core for each process execution. Since the
\texttt{IA32\_APERF}/\texttt{IA32\_AMERF} ratio is not usable during intervals
where a CPU core is powered down, we omit these sections from our frequency
ratio plots.\footnote{XXX detail the heuristic used to identify an idle core.}
Though simple, our plots highlight a number of interesting warmup behaviours:

\begin{figure}[tbp]
\includegraphics[width=.475\textwidth]{examples/slowdown1}
\caption{An example of slowdown, at in-process iteration \#199.}
\label{fig:examples:slowdown1}
\end{figure}

\begin{figure*}[tbp]
\makebox[\textwidth][c]{
\begin{minipage}{.475\textwidth}
\includegraphics[width=\textwidth]{examples/good1.pdf}
\end{minipage}
\begin{minipage}{.475\textwidth}
\includegraphics[width=\textwidth]{examples/good2.pdf}
\end{minipage}
}
\caption{Examples of traditional warmup behaviour. Since such warmup often completes
relatively quickly, the $x$-axis scale can make it hard to see when such warmup
completes. The inner graph zooms in on the relevant initial iterations to allow
close examination of the warmup in-process iterations. On the left-hand side,
warmup is complete by in-process iteration 1; on the right-hand side by around
iteration \#120.}
\label{fig:examples:trad}
\end{figure*}

\textbf{Flat} Some process executions, particularly the C benchmarks, have
constant in-process iteration times for the whole process execution.

\textbf{Warmup} Some process executions warmup up as per our
expectations. Figure~\ref{fig:examples:trad} shows some examples of traditional
warmup.

\textbf{Slowdown} \label{sub:slowdowns}
Some process executions exhibit `slowdown', where the performance of
in-process iterations drops over time. Figure~\ref{fig:examples:slowdown1} shows
an example where a sharp slowdown occurs.

\begin{figure}[tbp]
\includegraphics[width=.475\textwidth]{examples/cycles1}
\caption{An example of cyclic behaviour. In this case, the cycles span 6 in-process iterations,
and can be seen clearly in the inner graph (which zooms in on a sub-set of the in in-process
iterations).}
\label{fig:examples:cycles}
\end{figure}


\textbf{Cyclic} \label{sub:cyclic}
Some process executions exhibit cyclic behaviour, where in-process iteration times
repeat in a predictable pattern. 
Figure~\ref{fig:examples:cycles} shows
an example with a cycle of 6 in-process iterations.


\begin{figure}[t]
\includegraphics[width=.475\textwidth]{examples/many_phases}
\caption{An example of never-ending phase changes.}
\label{fig:examples:neverending}
\end{figure}

\textbf{No steady state} \label{sub:long}
Some process executions have distinct phases of execution but never seem to
settle on a single behaviour. Figure~\ref{fig:examples:neverending} shows an
example.


\begin{figure*}[t!]
\makebox[\textwidth][c]{
\includegraphics[width=\textwidth]{examples/inconsistent1.pdf}\\
}
\makebox[\textwidth][c]{~}  % only way I could add vertical space
\makebox[\textwidth][c]{
\includegraphics[width=\textwidth]{examples/inconsistent2.pdf}
}
\caption{Examples of inconsistent benchmarks.
The top row shows inconsistencies between different process
executions on the same machine. In this case the Fannkuch Redux benchmark
seems to follow two distinct patterns on different process executions,
sometimes going through a second warmup phase until around in-process iteration 375, and
other times not doing so (based on our data, these two patterns seem to occur
equally often). The bottom row shows inconsistencies between
machines.}
\label{fig:examples:inconsistent}
\end{figure*}

\textbf{Inconsistent Process Executions} \label{sub:inconsistent}
For some benchmark and VM pairings, different process executions behave
inconsistently. Sometimes this occurs on the same machine; sometimes between
different machines.
Figure~\ref{fig:examples:inconsistent} shows examples of both cases.


The core-cycle plots help us understand how VMs use, and how the OS schedules,
threads. Benchmarks running on single-threaded VMs are characterised by a high
cycle-count value on one core, and a near zero value on all other cores.
Further, single-threaded VMs may migrate to between cores within a process
execution. Figure~\edd{XXX} shows a process execution where the VM stays on the
same core for the whole process execution, whereas Figure~\edd{XXX} shows a
case where the VM migrates several times over the course of the process
execution. For multi-threaded VMs, the OS (as expected) schedules threads on
multiple cores simultaneously. This is characterised by consistently high
core-cycle counts for more than one core over the same timespan.
Figure~\edd{XXX} shows a process execution where multi-threading is evident.

In some cases, the wall-clock times vary over time, and the core-cycle counts
reflect these changes. Figure\edd{XXX} shows a case where there is a speed-up
in the wall-clock times, and at the same time the core-cycle measurements
decrease proportionally. This is indicative of the speed-up being a result of
the VM doing less work on the CPU, due to e.g. the execution of freshly JIT
compiled code. Some slowdowns and cycles can be explained in the same way:
Figure~\edd{XXX} shows a case where a slowdown in wall-clock times is reflected
by the core-cycle counts. Similarly, Figure~\edd{XXX} shows cyclic behaviour
reflected in the core-cycle counts. In some cases, however, changes in
wall-clock times are not reflected by the core-cycle counts. Figure~\edd{XXX}
shows an example. One possible explanation might be that the CPU frequency
changed mid-process-execution, however, as the frequency ratio plots show, the
ratio does not drop below 1, so this cannot be the case. For all of the data
collected, we did not observe the \texttt{IA32\_APERF/IA32\_AMERF} delta ratio
fall significantly below than 1 for a busy CPU core.

To better understand our results, we re-ran the PyPy and Hotspot benchmarks
with instrumentation mode enabled in \krun. This allows us to see when garbage
collection and JIT compilation occur. Although instrumentation mode performs
more I/O, if the run-sequence, core-cycle and frequency ratio plots look on the
whole the same as before, the instrumentation data is highly likely to
correspond. \edd{Add more here when we have some graphs}

\edd{Startup results in here somewhere?}

\subsection{Statistical Classification of Warmup Behaviours}

Having informally classified warmup behaviours, we now formally define these
behaviours using automated statistical methods, before proceeding to classify
all XXX in-process executions in our data.

\edd{formally define}

\textbf{Flat}

\textbf{Warmup}

\textbf{Slowdown}

\textbf{Cyclic}

\textbf{No steady state}

\edd{Is this para needed somewhere?}One common feature is \emph{outliers}: informally, these are in-process
iterations which are slower (or, less commonly, much faster) than neighbouring
in-process iterations. In general, we disregard outliers when classifying data
(i.e.~we pretend the outliers do not exist).
Figure~\ref{fig:examples:outliers1} shows a typical example, where in amongst
largely consistent timings, a single in-process iteration is substantially
slower than its neighbours.

\edd{Table showing results from automated analysis}

\begin{figure}[tbp]
\includegraphics[width=.475\textwidth]{examples/outliers1}
\caption{An example of an outlier, at in-process iteration \#1646.}
\label{fig:examples:outliers1}
\end{figure}


\section{The Effects of Compilation and GC}
\label{sec:deepdive}

\subsection{VM Instrumentation Mode}

For PyPy and Hotspot, \krun has an \emph{instrumentation mode}, which logs certain
VM events. We collect data that allows us to compute (for each in-process
iteration) how much time was spent in the garbage collector and in the JIT.
Our hope is that this will help to associate meaning to
steps and spikes in in-process iteration times. By default this mode is off,
since the instrumentation itself may interfere with measurements.
We will re-run a subset of the experiment under this mode after collecting the
main batch of results.


\section{Startup Time}
\label{sec:startup}


\section{Applying the Statistical Method to Existing Benchmark Suites}
\label{sec:existing}

The statistical method presented in Section \laurie{XXX} is not limited to data
produced from Krun. To demonstrate this, we have applied it to two standard
benchmark suites: DaCapo~\cite{dacapo06} (Java) and Octane~\laurie{cite?}
(JavaScript). In both cases, we ran 10 process executions and 2000 in-process
iterations on \bencherthree (without reboots, temperature control etc.).

We ran DaCapo on Graal and HotSpot. As it already has support for
altering the number of in-process executions, we used it without modification.
However, we were unable to run some benchmarks. \laurie{CF: can you list, and
briefly explain, the ones you disabled?}. We also disabled tradesoap, which
failed an internal validation check during our first `full' run on HotSpot:
although we have not subsequently observed this failure, we assume it could
reoccur, and have thus disabled it.

We ran Octane on SpiderMonkey (\#465d150b, a JIT compiling VM for JavaScript) and V8. 
We replaced its complex runner (which reported timings with a non-monotonic
microsecond timer) with a simpler alternative (using a monotonic millisecond
timer). Note that both runners run all Octane benchmarks in a single process-execution.
We also had to decide on an acceptable notion of `iteration'. Many of Octane's
benchmarks consist of a relatively quick `inner benchmark'; an `outer benchmark'
specifies how many times the inner benchmark should be run in order to make an
adequately long running benchmark (very roughly, around 1s on \bencherthree, for
example). We recorded 2000 iterations of the outer benchmark; our runner
fully resets the benchmark and the random number generator between each
iteration. The \texttt{box2d}, \texttt{gameboy}, \texttt{mandreel} benchmarks do
not properly reset their state between runs, leading to run-time errors we have
not been able to fix; \texttt{typescript}'s reset function, in contrast, is
overzealous, freeing constant data needed by all iterations, which we were able
to easily fix. When run for 2000 iterations, \texttt{CodeLoadClosure},
\texttt{pdfjs}, and \texttt{zlib} all fail due to memory leaks. We were able to
easily fix \texttt{pdfjs} by emptying a globally held list after each iteration,
(a patch has been submitted to, but not yet accepted, to Octane), but not the
others. We therefore include 12 of Octane's benchmarks (including lightly
modified versions of \texttt{pdfjs} and \texttt{typescript}).


\section{Threats to Validity}
\label{sec:threats}

While we have designed our experiment as carefully as possible, we do not
pretend to have controlled every possibly confounding variable. Indeed, our
experience when designing the experiment has been one of gradually uncovering
confounding variables whose existence we had not previously imagined. It
is inevitable that there are further confounding variables that we
do not know about; some of these may be controllable, although many may not be.
It is possible that confounding variables that we are not aware of have
coloured our results.

We have tried to gain a partial understanding of the effects of different
hardware on benchmarks by using benchmarking machines with the same OS but
different hardware. However, while the hardware between the two is
different, much more distinct hardware (e.g.~using a non-x86 architecture) is
available, and is more likely to uncover hardware-related differences.
However, hardware cannot always be controlled in isolation from software:
the greater the differences in hardware, the more likely that JIT compilers
compilers are to use different code paths (e.g.~different code generators and
the like). Put another way, an apples-to-apples comparison across very different
hardware is likely to be impossible, because the software being run will
change its behaviour.

We have not yet systematically tested whether rebuilding VMs effects warmup, an
effect noted by \kalibera. Our previous experience of JIT compilers suggests
that there is little effect in rebuilding such VMs when measuring peak
performance~\cite{barrett15approaches}. However, since measuring warm-up largely
involves measuring code that was not created by a JIT compiler, it is possible
that these effects may impact upon our experiment. To a very limited extent, the
rebuilding of VMs that occurred on our different VMs gives some small evidence
as to this effect, or lack thereof, but we will perform a deeper investigation
in the future.

The checksums we added ensure that, at a user-visible level, each benchmark
performs equivalent work in each language variant. However, it is impossible to
say whether each performs equivalent work at the lowest level or not. For
example, choosing to use a different data type in a language's core library may
substantially impact performance. There is also the perennial problem as to the
degree to which an individual language benchmark should maintain the structure
of other language's implementations: a benchmark for a given language could be
rewritten in a way that betters either or both of its warmup and peak
performance. From our perspective, this possibility is somewhat less important,
since we are more interested in the warmup patterns of reasonable programs,
whether they be the fastest possible or not. It is also possible that by
inserting checksums we have created unrepresentative benchmarks, though
this complaint could arguably be directed at the unmodified benchmarks too.

Although \krun does as much to control CPU clock speed as possible, modern CPUs
do not always respect operating system requests. Even on Linux, where we control
the CPU's P-state, we cannot guarantee that this fixes the CPU frequency: as
the Linux kernel documentation states, ``the idea that frequency can be set to a single
frequency is fiction for Intel Core processors''~\cite{pstate}. In
some cases, changes the CPU makes to its performance are detected and reported
by the operating system (e.g.~performance throttling due to potential
overheating); in other cases, changes may go undetected or unreported.
Despite this, our benchmarks show fairly predictable performance across
different hardware, suggesting that the effect of CPU performance changes may
not be significant in our case.

Although we have minimised the number of system calls that our in-process
iterations runners make, some still remain for taking measurements. For each
in-process iteration, we make two calls to \texttt{clock\_gettime()} to
read the start and finish time of each in-process iteration. On OpenBSD
\laurie{someone should check this on linux}, this can take an
unbounded amount of time as, indirectly, it enters a loop which only terminates
when a particular data-structure ceases to be modified, akin to a spin-lock. In
practise, \texttt{clock\_gettime} returns sufficiently quickly that this seems a
minor worry at most.
On Linux, we make additional calls to \texttt{open()}, \texttt{close()},
\texttt{lseek()} and \texttt{read()} to obtain performance counter values from
Model Specific Register (MSR) device nodes\footnote{We use a slightly modified
    \texttt{msr} device driver. The modification disables \texttt{capabilities}
so that we can access MSRs \emph{and} use \texttt{LD\_LIBRARY\_PATH} as a
non-root user.}. We found these calls to be more expensive than invoking system
calls, which is why wall-clock time measurements (which we consider to be the
most important readings) are the innermost measurements (see
Listing~\ref{lst:krun-measure}). \texttt{libkruntime.so} also holds open file
descriptors to device nodes for the duration of a process execution, thus
minimising the number of calls to \texttt{open()} and \texttt{close()}.

Reading a performance counter value from an MSR device node causes the
underlying device driver to execute an \texttt{RDMSR} instruction. To read
counters from other CPU cores than the current core, the driver must use
inter-processor interrupts (IPIs) to schedule \texttt{RDMSR} instructions
elsewhere. There is inherent latency in this mechanism, as it involves bus
communication and the interrupted core must save its registers to memory before
it can perform the requested work.

We investigated other timing mechanisms, most notably the Time Stamp Counter
(TSC) built into x86 processors. On older systems this regsister could be used
as a proxy for CPU frequency, but in modern systems the counter increments at a
fixed frequency and is thus not useful for our purposes.

In controlling confounding variables, our benchmarking environment necessarily
deviates from standard configurations. It is possible that in so doing, we have
created a system that shows warmup effects that few people will ever see in
practise.

We have identified a number of different styles of warmup (slowdown, cyclic,
etc.). However, we do not claim to have uncovered all possible warmup styles. It
is quite possible that other patterns exist that either do not show up in our
data, or which we have not detected.


\section{Discussion}
\label{sec:Discussion}

Although we are fairly experienced in designing and implementing
experiments, the experiment in this paper took more time and effort than we
expected. In part this is because there is limited precedent for such detailed
experiments. Investigating possible confounding variables, understanding how to
control them, and implementing the necessary checks, all took time. In many
cases, we had to implement small programs or systems to understand a variable's
effect (e.g.~that Linux allows a process to allocate memory beyond that
specified in the soft and hard \texttt{ulimit}).

In some cases, we found that seemingly reasonable settings had undesirable
effects. Most notably, we trialled ``pinning'' benchmarks to a set of idle CPU
cores. We experimented with two ways of doing this under Linux. The first,
\texttt{isolcpus}, worked for single threaded VMs, but multi-threaded VMs
suffered as much as a 3x slowdown. A
bug\footnote{\url{https://bugzilla.kernel.org/show_bug.cgi?id=116701}} in this
Linux kernel causes threads to be scheduled on the same core, thus defeating
the multi-threading used by some VMs (e.g. JRuby/Truffle) for compilation, GC,
etc. We were able to achieve the desired effect using a Linux ``cpuset shield''
occupying 3 of the 4 cores of our machines, however, we were worried that VMs
may attempt to schedule 4-cores worth of threads oblivious to the CPU affinity
of the cpuset.



\section{Related work}

\edd{paper on the effects of turbo boost: \url{https://www.cs.sfu.ca/~fedorova/papers/TurboBoostEvaluation.pdf}}

There are two works we are aware of which explicitly note unusual warmup
patterns. Gil et al.'s main focus is on non-determinism of process executions on
HotSpot, and the difficulties this raises in terms of providing reliable
benchmarking numbers~\cite{gil11microbenchmark}. In this process, they report at
least one benchmark (listBubbleSort) which on some process executions undergoes what we
would classify a slowdown. \kalibera note the
existence of what we have called cyclic behaviour (in the context of benchmarking,
they then require the user to
manually pick one part of the cycle for measurement~\cite{kalibera13rigorous}).


\section{Conclusions}
\label{sec:conclusion}

Warmup has always been an informally defined term~\cite{seaton15phd} and in this
paper we have shown cases where the definition fails to hold.
To the best of our knowledge, this paper is the first to classify different
`warmup' styles and note the relatively high frequency of non-traditional
classifications such as slowdown and never-ending phase changes.
However, as every keen student of history discovers, it is easier to destroy than to
create: we have not yet found an acceptable alternative definition of warmup.
Based on our experiences thus far, we think it unlikely that the different
styles of warmup we have seen can be captured in a single metric. We suspect it
is more likely that a number of different metrics will be needed to describe and
compare warmup styles.

\textbf{Acknowledgements:} We are particularly grateful to Vincent Knight
who helped put together the team that made this paper. We also thank (in alphabetical order) Dave Dice, Kenny
Gross, Tim Harris, Dave Ungar, Mario Wolczko for comments and suggestions; any
errors and infelicities are our own. This research was funded by the EPSRC
Cooler (EP/K01790X/1) grant and Lecture (EP/L02344X/1) fellowship.

\bibliographystyle{plain}
\bibliography{bib}

\end{document}

