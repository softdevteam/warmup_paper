\documentclass[10pt,preprint]{sigplanconf}

\usepackage[utf8]{inputenc}
% the following standard packages may be helpful, but are not required
%\usepackage{longtable}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{courier}
\usepackage[scaled]{helvet}
\usepackage{url}
\usepackage{listings}
\usepackage{enumitem}
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink
\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI

\usepackage{graphicx}
\usepackage{softdev}
\usepackage{amsmath}
\usepackage{mdwlist}
\usepackage{pifont}

\lstset{
	basicstyle=\tt\scriptsize,
	xleftmargin=2em,
    framexleftmargin=1.5em,
	numberstyle=\scriptsize\tt\color{gray},
	captionpos=b,
	escapeinside={{<!}{!>}},
}

\begin{document}

\title{An Investigation into the Warmup Behaviour of Various Virtual Machines}
\authorinfo{Edd Barrett}
           {Software Development Team\\ Department of Informatics\\ King's College London}
           {http://eddbarrett.co.uk/}
\authorinfo{Carl Friedrich Bolz}
           {Software Development Team\\ Department of Informatics\\ King's College London}
           {http://cfbolz.de/}
\authorinfo{Laurence Tratt}
           {Software Development Team\\ Department of Informatics\\ King's College London}
           {http://tratt.net/laurie/}

\maketitle


\begin{abstract}
Warmup is magic.
\end{abstract}

\section{Rough Notes}

XXX what is the hypothesis we are checking? how about "the traditional notion
of warmup is true", which we then reject?


- safety checks
  - are outliers deterministic across different processes? is the behaviour
    generally the same looking?
  - do some checks on other OS, machine architecture, frequency scaling independent

- threats to validity

\section{Introduction}
\label{sec:intro}

Recently there has been a boom in the number of emerging programming language
implementations. Some of the new implementations target already existing
languages (e.g. HHVM~\cite{XXX} and Truffle/JS ~\cite{XXX}), whereas others
target new languages altogether (e.g. Go~\cite{XXX}, Nim~\cite{XXX},
Rust~\cite{XXX} and Dart~\cite{XXX}). Regardless of the design goals of a new
language implementation, there is a shared consensus among language
implementers: that new language implementations must be performant.  This
consensus stems from the fact that the clock speeds of new CPUs have remained
fixed for the last few years. This means that poor program performance can no
longer be overcome by simply using a faster CPU, and thus there is a great deal
of pressure for language implementers to deliver high performance systems.

With this new focus on building fast language implementations, VM and
compiler engineers have started paying close attention to the performance of
their systems. It is now not uncommon for projects to measure performance over
the development life-cycle of their system;  using benchmarking to spot
performance regressions and to take action should they arise. This is a step
in the right direction; commendable perhaps, however benchmarking is not a
trivial subject.

The problem with benchmarking is that there are many different variables that
must be considered. How many iterations should be run? Does running on
different hardware affect the outcome? Does using a different compiler to build
our VM impact performance? Is this benchmark representative of a real program?
The list goes on. Benchmarking is further complicated by \emph{Just In Time}
(JIT) compilation techniques that are often deployed in modern language
implementations. Typically the initial iterations of a benchmark running on a
JIT tend to be much slower than later iterations. This is due to the fact that
the JIT must profile and compile at runtime in order to generate fast code.
The term \emph{peak performance} is often used to describe the point at which
the fast code has been generated, and it is common for experimenters to
disregard data prior to this point when benchmarking.

Although the concept of JIT warmup is widely known, few people have actually
thought in detail about how we should account for these effects when
benchmarking. Commonly, a mean time for a benchmark derived by running a
benchmark a few times (less than 100) in a single process, before ``chopping
JIT warmup''. Although more intricate methodologies have recently
proposed~\cite{XXX}, uptake has been slow, and it is not clear if they
properly account for JIT warmup either.

In this paper we scrutinise benchmarking methodologies and question if we --
as a field -- are correctly measuring the performance of our JIT compilers,
or if we are fooling ourselves by publishing biased results due to a
misunderstanding of warmup behaviours. Our contributions are as follows:

\begin{itemize}
\item We first discuss the traditional notion of warmup behaviours that we have
come to expect from programming language VMs.
\item We describe an experiment designed to measure faithfully the true
warmup behaviours of industry standard VMs.
\item We analyse the results of our experiment in depth, showing a disparity
between the expected and actual warmup behaviours of modern VMs.
\item \edd{We discuss what we need to do about this?}
\end{itemize}


\section{Conceptions of Warmup}
\label{sec:Conceptions of Warmup}

- regular notions of warmup
  - describe what warmup is intuitively
  - "a phase of slower execution time when using JITting VMs that is followed
    by faster executions randomly distributed around a constant mean"

The term ``warmup'' refers to the way in which language runtimes tend to take
time to reach a steady state of peak performance.~\footnote{Although simple
interpreters and ahead of time compiled programs (that don't use a JIT) may
have a small initial startup costs. e.g. when the runtime linker relocates the
binary.} The term is mostly used when talking about JIT compilers. A typical VM
with a JIT begins executing a program in a profiling interpreter. Information
gathered through profiling is then used as a heuristic to decide which portions
of the program are good candidates for compilation to fast native code. Once a
candidate has been compiled, the VM uses the fast native code in the future
(instead of the slow interpreter), thus improving performance.  Although
interpretation, profiling and compilation all come with a performance hit,
ideally there should be a point at which the VM has all of the binary code it
needs to execute the whole program natively. At this point, interpretation,
profiling and compilation should no longer occur, and thus peak performance is
met. The graph in Figure~\ref{XXX} demonstrates this ideal scenario for a
fictional benchmark run repeatedly within a single process.



\section{Benchmarking Methodology}
\label{sec:Benchmarking Methodology}

- describe how we did the measurement
  - all the precautions
  - XXX Edd's job


\section{Results}
\label{sec:Results}

- analysis of the results
  - found regular results
  - anomalies
  - outliers
    - need a definition, n sigma events? outside of the 99\% confidence interval?
  - slowdowns
    - first few iterations are faster than eventual mean
  - cyclic behaviour
    - every n iterations show really regular behaviour
  - late phase changes
    - definition? "I know it when I see it"

\section{Discussion}
\label{sec:Discussion}

  - discussion:
    - need to give up naive definition of warmup
    - unrealistic to get rid of these anomalies
    - some of benchmarking wisdom is wrong in the presence of this stuff


\section{Conclusions}
\label{sec:conclusion}

\bibliographystyle{abbrvnat}
\bibliography{bib}


\end{document}

