\documentclass[preprint]{sigplanconf}

\usepackage[utf8]{inputenc}
% the following standard packages may be helpful, but are not required
\usepackage{longtable}
%\usepackage{mathtools}
%\usepackage{multicol}
%\usepackage{multirow}
\usepackage{booktabs}
%\usepackage{courier}
%\usepackage[scaled]{helvet}
%\usepackage{url}
\usepackage{listings}
%\usepackage{enumitem}
%\usepackage{mdwlist} % tighter description environment (starred)
%
\usepackage{graphicx}
\usepackage{softdev}
\usepackage{amsmath}
%\usepackage{mdwlist}
%\usepackage{pifont}
\usepackage{xspace}

\newcommand{\kalibera}{Kalibera \& Jones\xspace}
\newcommand{\krun}{Krun\xspace}
\newcommand{\hypone}{H1\xspace}
\newcommand{\hyptwo}{H2\xspace}
\newcommand{\binarytrees}{\emph{binary trees}\xspace}
\newcommand{\richards}{\emph{Richards}\xspace}
\newcommand{\spectralnorm}{\emph{spectralnorm}\xspace}
\newcommand{\nbody}{\emph{n-body}\xspace}
\newcommand{\fasta}{\emph{fasta}\xspace}
\newcommand{\fannkuch}{\emph{fannkuch redux}\xspace}
\newcommand{\bencherthree}{Linux1/i7-4709K\xspace}
\newcommand{\bencherfive}{Linux2/i7-4790\xspace}
\newcommand{\benchersix}{OpenBSD/i7-4790\xspace}
\newcommand{\bencherseven}{ARM\edd{XXX name properly if used}\xspace}

\lstset{
    basicstyle=\tt\scriptsize,
    xleftmargin=2em,
    framexleftmargin=1.5em,
    numberstyle=\scriptsize\tt\color{gray},
    captionpos=b,
    escapeinside={{<!}{!>}},
}

\SDShowCommentTags{default}  %final

\begin{document}

\title{Virtual Machine Warmup Blows Hot and Cold}

\authorinfo{Edd Barrett}
{Software Development Team, Department of Informatics, King's College London}{\texttt{http://eddbarrett.co.uk/}}

\authorinfo{Carl Friedrich Bolz}
{Software Development Team, Department of Informatics, King's College London}{\texttt{http://cfbolz.de/}}

\authorinfo{Rebecca Killick}
{Department of Mathematics and Statistics, University of Lancaster}{\texttt{http://www.lancs.ac.uk/\~{}killick/}}

\authorinfo{Vincent Knight}
{School of Mathematics, Cardiff University}{\texttt{http://vknight.org/}}

\authorinfo{Sarah Mount}
{Software Development Team, Department of Informatics, King's College London}{\texttt{http://snim2.org/}}

\authorinfo{Laurence Tratt}
{Software Development Team, Department of Informatics, King's College London}{\texttt{http://tratt.net/laurie/}}


\maketitle

\section{Introduction}
\label{sec:intro}

Many modern languages are implemented as Virtual Machines (VMs) which use a
Just-In-Time (JIT) compiler to translate `hot' parts of a program into efficient
machine code at run-time. Since it takes time to determine which parts of the
program are hot, and then compile them, programs which are JIT compiled are
said to be subject to a \emph{warmup} phase. The traditional view of
JIT compiled VMs is that program execution is slow during the warmup phase, and
fast afterwards, when \emph{peak performance} is said to have been reached
(see Figure~\ref{fig:trad} for a simplified view of this).
This traditional view underlies most benchmarking of JIT compiled VMs, which
generally aim to measure peak performance.
Typically, benchmarks are run repeatedly in a single process, then data prior
to peak performance is discarded.

The aim of this paper is to test the following hypothesis:
\begin{description}
  \item[\hypone] Small, deterministic programs exhibit traditional warmup behaviour.
\end{description}
In order to test this hypothesis, we present a carefully designed
experiment where a number of simple benchmarks are run on a variety of
VMs for a large number of \emph{in-process iterations} and repeated using fresh
\emph{process executions} (i.e.~each process execution runs multiple in-process
iterations). We deliberately treat VMs as black boxes: we simply run benchmarks
and record timing data.

\begin{figure}[t]
\centering
\includegraphics[width=.5\textwidth]{img/picturebook_warmup}
\caption{The traditional notion of warmup: a program starts slowly executing in
an interpreter; once hot parts of the program are identified, they are
translated by the JIT compiler to machine code; at this point warmup
is said to have completed, and peak performance reached.}
\label{fig:trad}
\end{figure}

While some benchmarks on some VMs run as per
traditional expectations, we found a number of surprising cases. At
the most extreme, some benchmarks never warm up, staying at their initial performance
levels indefinitely and some even slowdown. Of the eight
VMs we looked at, none consistently warmed under the traditional model.

Our results clearly invalidate H1: the traditional view of warmup is no longer
valid. We are not aware that anyone has systematically noted this
problem before, let alone take it into account when benchmarking. This suggests
that many published VM benchmarks (including our own) may have presented
results which are misleading in some situations.

We believe that accurate VM benchmarking is paramount, for both VM authors, and
for many end users. VM authors need to know if optimisations have an effect
distinguishable from noise (and many optimisations have only a small effect).
Similarly, end-users with latency sensitive workloads (e.g. games or other soft
real-time systems) rely upon accurate benchmarking during their evaluation
phase. Our results suggest that current benchmarking methodology is leading
these parties astray.

\section{Background}
\label{sec:warmup}

When a program begins running on a JIT compiled VM, it is typically (slowly)
interpreted; once `hot' (i.e.~frequently executed) loops or methods are
identified, they are compiled into machine code; and subsequent
executions of those loops or methods use (fast) machine code rather than the
(slow) interpreter. Once machine code generation has completed, the VM is
traditionally said to have finished warming up, and the program to be executing
at peak performance.\footnote{This traditional notion applies equally to VMs
that perform immediate compilation instead of using an interpreter, and to
those VMs which have more than one layer of JIT compilation (later JIT
compilation is used for `very hot' portions of a program, and tolerates slower
compilation time for better machine code generation).}

Figure~\ref{fig:trad} illustrates a
program subject to the conventional model of warmup. Exactly how long warmup
takes is highly dependent on
the program and the JIT compiler, but this basic assumption about the
performance model is shared by every JIT compiling
VM~\cite{kalibera13rigorous}.

Benchmarking of JIT compiled VMs typically focusses on peak
performance. in large part because the widespread assumption has been that
warmup is both fast and inconsequential to users. With that assumption in mind, the
methodologies used are typically straightforward: benchmarks are run for a number
of in-process iterations within a single VM process execution.
The first $n$ in-process iterations are then discarded, on the basis that warmup
will have completed at some point before $n$. It is common for
$n$ to be a hard-coded number, e.g. 5. The more sophisticated \kalibera
benchmarking methodology~\cite{kalibera12quantifying,kalibera13rigorous}
(recently used in~\cite{barrett15approaches,grimmer15dynamically}) improves
upon this by having the user manually inspect run-sequence plots (or trace
plots) for each process execution. The method also suggests a method for
dealing with benchmarks which are cyclic in nature (e.g. that produce a
saw-tooth wave when plotted).

While the \kalibera methodology is certainly an improvement over
straightforward benchmarking methodologies,
our experience has been that there remain cases where it is hard to produce
satisfying benchmarking statistics. Crucially, the methodology does not
provide a firm way of determining when warmup has completed. Because of this
``determining when a system has warmed up, or even providing a
rigorous definition of the term, is an open research problem''~\cite{seaton15phd}.


\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}

